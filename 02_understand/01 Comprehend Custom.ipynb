{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Comprehend Custom\n",
    "\n",
    "[Amazon Comprehend](https://aws.amazon.com/comprehend/) provides pre-trained and user-trainable tools for discovering insights and relationships from text.\n",
    "\n",
    "Pre-trained [features](https://aws.amazon.com/comprehend/features/) include:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Topic analysis\n",
    "- Keyphrase extraction\n",
    "- Syntax analysis\n",
    "- Entity recognition\n",
    "- Medical entity recognition and ontology linking\n",
    "- Language detection\n",
    "- PII detection and masking\n",
    "- Event data extraction\n",
    "\n",
    "With [Comprehend Custom](https://docs.aws.amazon.com/comprehend/latest/dg/auto-ml.html), users can train custom **classification** or **entity detection** models on domain-specific datasets.\n",
    "\n",
    "In this example, we'll demonstrate the basics by training a **custom topic classifier** on the Yahoo answers corpus cited in the paper [Text Understanding from Scratch](https://arxiv.org/abs/1502.01710) by Xiang Zhang and Yann LeCun.\n",
    "\n",
    "## Fetching the data\n",
    "\n",
    "The raw dataset is available on the [AWS Open Data Registry](https://registry.opendata.aws/fast-ai-nlp/), but we'll use a pre-process version associated with [\"Building a custom classifier using Amazon Comprehend\"](https://aws.amazon.com/blogs/machine-learning/building-a-custom-classifier-using-amazon-comprehend/) - from the AWS Machine Learning blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aws-ml-blog/artifacts/comprehend-custom-classification/comprehend-test.csv ./data/comprehend/\n",
    "!aws s3 cp s3://aws-ml-blog/artifacts/comprehend-custom-classification/comprehend-train.csv ./data/comprehend/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "For this workshop, our S3 bucket and IAM roles have already been set up. We just need to look up their names, and then upload the prepared data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "\n",
    "# The S3 bucket has already been created & named for us:\n",
    "account_id = os.environ[\"AWS_ACCOUNT_ID\"]\n",
    "region = os.environ[\"AWS_DEFAULT_REGION\"]\n",
    "bucket_name = f\"comprehend-custom-{account_id}-{region}\"\n",
    "\n",
    "comprehend = boto3.client(\"comprehend\")\n",
    "s3 = boto3.resource(\"s3\")\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "print(f\"Using S3 bucket: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.Object(f\"data/comprehend-train.csv\").upload_file(f\"data/comprehend/comprehend-train.csv\")\n",
    "bucket.Object(f\"data/comprehend-test.csv\").upload_file(f\"data/comprehend/comprehend-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Comprehend Execution Role has also been created already, but we need to look it up:\n",
    "ssm = boto3.client(\"ssm\")\n",
    "ssm_response = ssm.get_parameters(Names=[\"/workshop/ComprehendRoleArn\"])\n",
    "\n",
    "try:\n",
    "    comprehend_param = next(\n",
    "        param for param in ssm_response[\"Parameters\"]\n",
    "        if param[\"Name\"] == \"/workshop/ComprehendRoleArn\"\n",
    "    )\n",
    "    comprehend_role_arn = comprehend_param[\"Value\"]\n",
    "    print(comprehend_role_arn)\n",
    "except StopIteration as e:\n",
    "    print(ssm_response)\n",
    "    raise ValueError(\"Couldn't retrieve Comprehend Execution Role from SSM\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Comprehend Model\n",
    "\n",
    "With the data uploaded to S3, we're ready to create or Comprehend Custom model.\n",
    "\n",
    "You could follow this process either through the [Comprehend Console](https://console.aws.amazon.com/comprehend/v2/home?#classification) or by running the code cells below.\n",
    "\n",
    "If you create your classifier in the console instead, just be sure to declare the `cls_arn` in this notebook (available on the classifier details screen in the console)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cls_resp = comprehend.create_document_classifier(\n",
    "    DocumentClassifierName=\"yahoo-answers\",\n",
    "    DataAccessRoleArn=comprehend_role_arn,\n",
    "    InputDataConfig={\n",
    "        \"DataFormat\": \"COMPREHEND_CSV\", # (Or \"AUGMENTED_MANIFEST\" for SM Ground Truth)\n",
    "        \"S3Uri\": f\"s3://{bucket_name}/data/comprehend-train.csv\",\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': f\"s3://{bucket_name}/training-output\",\n",
    "    },\n",
    "    LanguageCode=\"en\", # See also 'es'|'fr'|'de'|'it'|'pt'|'ar'|'hi'|'ja'|'ko'|'zh'|'zh-TW'\n",
    ")\n",
    "\n",
    "print(create_cls_resp)\n",
    "cls_arn = create_cls_resp[\"DocumentClassifierArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier training can take time, depending on the size and complexity of your dataset, so the above operation is asynchronous. Below, we set up a polling loop to wait for the classifier to become ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "pending_statuses = { \"SUBMITTED\", \"TRAINING\", \"DELETING\", \"STOP_REQUESTED\" }\n",
    "success_statuses = { \"TRAINED\" }\n",
    "fail_statuses = { \"STOPPED\", \"IN_ERROR\" }\n",
    "\n",
    "while True:\n",
    "    cls_desc = comprehend.describe_document_classifier(\n",
    "        DocumentClassifierArn=cls_arn,\n",
    "    )\n",
    "    status = cls_desc[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    if status in success_statuses:\n",
    "        print(f\"\\nReady: {status}\")\n",
    "    elif status in fail_statuses:\n",
    "        raise ValueError(f\"Entered fail state {status}\")\n",
    "    elif status not in pending_statuses:\n",
    "        raise ValueError(f\"Entered unexpected state {status}\")\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the classifier is trained, you'll be able to see a [range of metrics](https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html) describing its performance - visible through the Comprehend Console or via APIs (see the `cls_desc` variable from above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running batch jobs\n",
    "\n",
    "Once the model is trained (regardless of whether it was a classifier or entity detector), we can either:\n",
    "\n",
    "- Deploy it to an endpoint for real-time inference, or\n",
    "- Run batch jobs to process defined datasets\n",
    "\n",
    "In general, using the right deployment type for your use case will be more economical and simpler to orchestrate. See the [Comprehend pricing page](https://aws.amazon.com/comprehend/pricing/) for details on how each option is charged.\n",
    "\n",
    "In this example, we already have a test dataset and would like to review model performance on it - so we'll set up a batch job:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "aws comprehend start-document-classification-job --document-classifier-arn arn:aws:comprehend:us-east-1:123456789012:document-classifier/yahoo-answers --input-data-config S3Uri=s3://123456789012-comprehend/comprehend-test.csv,InputFormat=ONE_DOC_PER_LINE --output-data-config S3Uri=s3://123456789012-comprehend/InferenceOutput/ --data-access-role-arn arn:aws:iam::123456789012:role/ComprehendBucketAccessRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_job_resp = comprehend.start_document_classification_job(\n",
    "    #JobName='string',\n",
    "    DocumentClassifierArn=cls_arn,\n",
    "    InputDataConfig={\n",
    "        \"S3Uri\": f\"s3://{bucket_name}/data/comprehend-test.csv\",\n",
    "        \"InputFormat\": \"ONE_DOC_PER_LINE\",\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        \"S3Uri\": f\"s3://{bucket_name}/results\",\n",
    "    },\n",
    "    DataAccessRoleArn=comprehend_role_arn,\n",
    ")\n",
    "\n",
    "print(create_job_resp)\n",
    "job_id = create_job_resp[\"JobId\"]\n",
    "# Also JobStatus: 'SUBMITTED'|'IN_PROGRESS'|'COMPLETED'|'FAILED'|'STOP_REQUESTED'|'STOPPED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with training, this is an asynchronous process so we'll have to wait for the job to finish before the results are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "pending_statuses = { \"SUBMITTED\", \"IN_PROGRESS\", \"STOP_REQUESTED\" }\n",
    "success_statuses = { \"COMPLETED\" }\n",
    "fail_statuses = { \"STOPPED\", \"FAILED\" }\n",
    "\n",
    "while True:\n",
    "    job_desc = comprehend.describe_document_classification_job(\n",
    "        JobId=job_id,\n",
    "    )\n",
    "    status = job_desc[\"DocumentClassificationJobProperties\"][\"JobStatus\"]\n",
    "    if status in success_statuses:\n",
    "        print(f\"\\nReady: {status}\")\n",
    "    elif status in fail_statuses:\n",
    "        raise ValueError(f\"Entered fail state {status}\")\n",
    "    elif status not in pending_statuses:\n",
    "        raise ValueError(f\"Entered unexpected state {status}\")\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing batch results\n",
    "\n",
    "Per [the Comprehend Developer Guide](https://docs.aws.amazon.com/comprehend/latest/dg/how-class-run.html), output from classification jobs is a **compressed archive** containing a **single JSON-lines result file**.\n",
    "\n",
    "We can download the result from S3 to this notebook, and extract the tarball:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket_name/results/$job_id/output/output.tar.gz ./data/comprehend/results/output.tar.gz\n",
    "\n",
    "!cd data/comprehend/results && tar -xvzf output.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line of this file is a JSON object corresponding to one of our input test documents, so for example we can read the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/comprehend/results/output/output.jsonl\", \"r\") as f:\n",
    "    for ix, line in enumerate(f):\n",
    "        res = json.loads(line)\n",
    "        print(f\"Doc result:\\n{json.dumps(res, indent=2)}\")\n",
    "\n",
    "        if ix >= 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
